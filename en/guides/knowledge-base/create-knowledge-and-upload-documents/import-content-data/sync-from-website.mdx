---
title: Use Websites as Data Source to Quick Create Knowledge
sidebarTitle: Import from Websites
---

When quick-creating a knowledge base, you can use websites as its data source, importing specific webpages and synchronizing them when their content is updated.

## Import Webpages

Choose from the available website crawler providers to fetch webpages from the target site and import them into a knowledge base.

<Tabs>
    <Tab title="Jina Reader">
        <Steps>
            <Step title="Install the Jina Reader Plugin">
                Go to **Settings** > **Data Source**, then find and install the Jina Reader data source plugin from the recommended list.
            </Step>
            <Step title="Complete Authorization">
                Click **+ Configure** to authorize Dify to access Jina Reader via API key.
            </Step>
            <Step title="Import Webpages">
                1. Click **Create** > **Create Knowledge**, then select **Sync from website** as the data source.

                2. Choose Jina Reader as the provider, enter the target website URL, and configure the crawling options, including:
    
                    - Whether to crawl sub-pages

                    - Whether to use a sitemap for crawling
                        > When enabled, if the target website has a sitemap at `https://example.com/sitemap.xml`, Jina Reader will crawl pages listed in the sitemap instead of following links from the target URL.
    
                    - The maximum number of pages to crawl

                3. Click **Run** to start crawling the website. After the crawl finishes, select the webpages you want to import.
            </Step>
        </Steps>
    </Tab>
    <Tab title="Firecrawl">
        <Steps>
            <Step title="Install the Firecrawl Plugin">
                Go to **Settings** > **Data Source**, then find and install the Firecrawl data source plugin from the recommended list.
            </Step>
            <Step title="Complete Authorization">
                Click **+ Configure** to authorize Dify to access Firecrawl via API key.
            </Step>
            <Step title="Import Webpages">
                1. Click **Create** > **Create Knowledge**, then select **Sync from website** as the data source.

                2. Choose Firecrawl as the provider, enter the target website URL, and configure the crawling options, including:
    
                    - Whether to crawl sub-pages

                    - The maximum number of pages to crawl

                    - The maximum crawl depth (how many levels of links Firecrawl will follow from the target URL)
    
                        > For example, if the target URL is `https://dify.ai/`:
                        > - Depth 0: Only crawl `https://dify.ai/` itself.
                        > - Depth 1: Crawl `https://dify.ai/` and the pages directly linked from it, such as `https://dify.ai/pricing` or `https://dify.ai/blog`.
                        > - Depth 2: Crawl all of the above, plus pages one level deeper, such as `https://dify.ai/blog/article-1`.
    
                    - Paths to exclude or include
    
                        > For example, if you set `Exclude paths` to `https://dify.ai/blog/*`, any pages whose URLs start with `https://dify.ai/blog/` will be skipped, such as:
                        > - `https://dify.ai/blog`
                        > - `https://dify.ai/blog/article-1` 

                    - Whether to extract only the main content (excluding headers, navigation, footers, and other layout elements)

                3. Click **Run** to start crawling the website. After the crawl finishes, select the webpages you want to import.
            </Step>
        </Steps>
    </Tab>
    <Tab title="WaterCrawl">
        <Steps>
            <Step title="Install the WaterCrawl Plugin">
                Go to **Settings** > **Data Source**, then find and install the WaterCrawl data source plugin from the recommended list.
            </Step>
            <Step title="Complete Authorization">
                Click **+ Configure** to authorize Dify to access WaterCrawl via API key.
            </Step>
            <Step title="Import Webpages">
                1. Click **Create** > **Create Knowledge**, then select **Sync from website** as the data source.

                2. Choose WaterCrawl as the provider, enter the target website URL, and configure the crawling options, including:
    
                    - Whether to crawl sub-pages
    
                    - The maximum number of pages to crawl

                3. Click **Run** to start crawling the website. After the crawl finishes, select the webpages you want to import.
            </Step>
        </Steps>
    </Tab>
</Tabs>
<Tip>
    After a knowledge base is created, you can add more webpages at any time from any website using any of the available providers.
</Tip>

## Synchronize Imported Webpages

If the content of an imported webpage is updated, you can synchronize the changes by clicking **Sync** for the corresponding webpage.

<Note>
    For knowledge bases using the **High Quality** index method, synchronization requires re-embedding, which consumes tokens from the embedding model.
</Note>

![Synchronize Imported Webpages](/images/sync_webpages.png)

{/*
Contributing Section
DO NOT edit this section!
It will be automatically generated by the script.
*/}

---

[Edit this page](https://github.com/langgenius/dify-docs/edit/main/en/guides/knowledge-base/create-knowledge-and-upload-documents/import-content-data/sync-from-website.mdx) | [Report an issue](https://github.com/langgenius/dify-docs/issues/new?template=docs.yml)

