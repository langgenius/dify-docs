---
title: Deploy Dify Docs MCP Service and Chat with Documentation
description: Learn how to deploy a local documentation MCP server and chat with documentation using AI.
---

Model Context Protocol (MCP) is an open standard developed by Anthropic. It provides standardized interfaces for AI models to interact with external tools and data sources. AI models can access software tools and data through this protocol.

This guide shows how to deploy a local MCP server using the [dify-docs repository](https://github.com/langgenius/dify-docs). This enables natural conversations between LLMs and documentation.

<Info>
    For detailed information about the MCP protocol, please refer to [Model Context Protocol](https://docs.anthropic.com/en/docs/agents-and-tools/mcp).
</Info>

<Accordion title="How It Works">
**1. Analyze the Question**

The AI receives a question and analyzes the concepts and technical details involved. It determines whether it needs to consult Dify documentation.

**2. Generate Search Keywords**

When the AI can't answer from its existing knowledge and needs to reference documentation, it automatically generates search keywords and query strategies. For "How to integrate OpenAI GPT-4 model in Dify", it searches for "model configuration", "API integration", "GPT-4 setup".

**3. Retrieve Documentation Content**

The MCP server receives these queries, searches through Dify's documentation library, and returns the most relevant content.

**4. Integrate and Generate Answer**

The AI organizes the retrieved information and generates an accurate, detailed response.

**5. Multi-round Search Optimization**

If the initial search doesn't provide enough information, the AI refines its search strategy and performs additional queries until it gets a comprehensive answer.
</Accordion>

## Prerequisites

- Node ≥ 16.0
- Git
- MCP client (such as Claude Desktop, VS Code, etc.)

## Installation Steps

### 1. Clone Repository

Clone the Dify documentation repository locally:

```bash
git clone https://github.com/langgenius/dify-docs.git
```

This repository serves as the data source for the MCP server.

### 2. Initialize MCP Server

Navigate to the documentation directory and install the MCP server:

```bash
cd dify-docs
npx mint-mcp add dify-6c0370d8
```

This command performs the following operations:
- Creates a local MCP server instance
- Analyzes and indexes documentation content
- Configures necessary dependencies

Installation takes a few minutes, depending on your network connection.

### 3. Configure MCP Server

<Info>
    **MCP Client and Server Relationship:**
    
    MCP servers provide knowledge and tools. They handle query requests, manage data resources, and execute functions. Clients are user interfaces—typically AI applications or chat tools. They receive user questions, forward queries to MCP servers, and return integrated responses.
</Info>

During installation, you'll select an MCP client. **Using Claude Desktop as an example**, the tool generates the corresponding configuration information.

![](https://assets-docs.dify.ai/2025/05/722c689b1c27d86c183c09dfda7a6a39.png)

After installation completes, add the MCP server in Claude Desktop settings. Go to **settings**, add the server address and port in the MCP server configuration section, save the configuration, and restart the application.

![](https://assets-docs.dify.ai/2025/05/a332247103fb9a7dcf5b1a891979ce86.png)

## Conversing with Documentation

Once configured, you can have conversations with Dify documentation. This supports natural language queries—no need to search page by page or match keywords precisely.

For example, ask "I want to create a customer service chatbot—should I choose Workflow or Chatflow?". The system explains the differences between both options and provides recommendations for customer service scenarios.

![](https://assets-docs.dify.ai/2025/05/10d6a23693e894b2cbac85946eb22da5.png)

For technical questions like "How do I modify the Dify port number?", the AI provides specific steps:

![](https://assets-docs.dify.ai/2025/05/e83881dbe69ea135760c4e04f6a7ce98.png)

The MCP approach lets you describe problems in natural language. Even when you're unsure about specific technical terms, you can describe what you want to achieve and get relevant information.

## Comparison with RAG Methods

| Feature | RAG (Retrieval Augmented Generation) | MCP (Model Context Protocol) |
|---------|-------------------------------------|-----------------------------|
| Principle | Splits documents into text chunks and builds indexes using vector embeddings. When users ask questions, the system calculates similarity between questions and text chunks, then selects relevant chunks as context for generating answers. | AI actively generates query strategies based on question requirements. Supports multi-round interactive queries and adjusts subsequent searches based on initial results. |
| Advantages | Fast processing speed, suitable for large-scale static documents. | - Can access complete document content, supports cross-chapter complex queries<br/>- No complex reprocessing after document updates—simply regenerate MCP service<br/>- Provides coherent and complete answers |
| Limitations | - Static document splitting may fragment related information<br/>- Vector similarity-based retrieval may miss semantically related but lexically different content<br/>- Context window limitations—only generates answers based on specific chnks | - Requires significant token consumption, higher cost<br/>- Relies on LLM's query strategy generation capability, may affect retrieval accuracy<br/>- Multi-round interactive queries may lead to longer response times<br/>- Requires additional MCP server deployment and maintenance costs |
