---
title: Expert Mode for Prompt Engineering (Discontinued)
---

When creating an app on Dify, the default orchestration mode is **Simple Mode**, which is ideal for non-technical users who want to quickly create applications like a company knowledge base chatbot or an article summarizer. Using **Simple Mode**, you can orchestrate pre-prompt phrases, add variables, and context with simple steps to publish a complete application.

However, if you are a technical user proficient in using **OpenAI's** **Playground** and want to create a learning tutor application that requires embedding different contexts and variables into the prompts for various teaching modules, you can choose **Expert Mode**. In this mode, you can freely write complete prompts, including modifying built-in prompts, adjusting the position of context and chat history within the prompts, and setting necessary parameters. If you are familiar with both Chat and Complete models, **Expert Mode** allows you to quickly switch between these models to meet your needs, and both are suitable for conversational and text generation applications.

Before you start experimenting with the new mode, you need to know the essential elements of **Expert Mode**:

- **Text Completion Model**

  When selecting a model, the name with COMPLETE on the right is a text completion model. This model accepts a free-form text string called a "prompt" and generates a text completion that tries to match any context or pattern you give it. For example, if your prompt is: "As Descartes said, I think therefore," it will likely return "I am" as the completion.

- **Chat Model**

  When selecting a model, the name with CHAT on the right is a chat model. This model takes a list of messages as input and returns a generated message as output. Chat models use chat messages as input and output, including three types of messages: SYSTEM, USER, and ASSISTANT.
  
- **Stop Sequences**

  These are specific words, phrases, or characters used to signal the LLM to stop generating text.

- **Content Blocks in Expert Mode Prompts**

  In an app configured with a dataset, the user inputs a query, and the app uses this query as a retrieval condition for the dataset. The retrieved results replace the `context` variable, allowing the LLM to reference the context content to provide an answer.

- **Initial Template**

  In **Expert Mode**, before formal orchestration, the prompt box provides an initial template that you can modify to make more customized requests to the LLM. Note: There are differences based on the type of application and mode.

## Comparison of Two Modes

| Comparison Dimension                                | Simple Mode          | Expert Mode                                                   |
| --------------------------------------------------- | --------------------- | -------------------------------------------------------------- |
| Built-in Prompt Visibility                          | Encapsulated and Invisible | Open and Visible                                              |
| Automatic Orchestration                             | Available            | Unavailable                                                   |
| Difference in Text Completion and Chat Model Selection | None                 | Different orchestration after selecting models                |
| Variable Insertion                                  | Available            | Available                                                     |
| Content Block Validation                            | None                 | Available                                                     |
| SYSTEM / USER / ASSISTANT Message Type Orchestration | None                 | Available                                                     |
| Context Parameter Settings                          | Configurable         | Configurable                                                  |
| View PROMPT LOG                                     | View full prompt log | View full prompt log                                          |
| Stop Sequences Parameter Settings                   | None                 | Configurable                                                  |

<Warning>
After modifying prompts and publishing the application in **Expert Mode**, you cannot return to **Simple Mode**.
</Warning>

## Operating Instructions

### 1. How to Enter Expert Mode

After creating an application, you can switch to **Expert Mode** on the prompt orchestration page, where you can edit the complete application prompts.

![Expert Mode Entry](../../../../.gitbook/assets/专家模式.png)

### 2. Modify Inserted Context Parameters

In both **Simple Mode** and **Expert Mode**, you can modify the parameters for the inserted context, including **TopK** and **Score Threshold**.

<Warning>
Note that the built-in prompt containing `{{#context#}}` will only be displayed in **Expert Mode** after uploading the context.
</Warning>

![Context Parameter Settings](../../../../.gitbook/assets/参数设置.png)

### 3. Set Stop Sequences

We do not want the LLM to generate unnecessary content, so specific words, phrases, or characters need to be set to inform the LLM to stop generating text.

### 4. Quick Insert Variables and Content Blocks

In **Expert Mode**, you can type "`/`" in the text editor to quickly bring up content blocks to insert into the prompt. You can also type "`{`" to quickly insert a list of previously created variables.

![Shortcut Key “/”](../../../../.gitbook/assets/快捷键.png)

### 5. Input Pre-prompt

The initial template of the system's prompt provides necessary parameters and LLM response requirements. Developers need to edit and insert the pre-prompt into the built-in prompt.

### 6. Debug Logs

During orchestration debugging, you can view the complete prompt to confirm whether the input variable content, context, chat history, and query content meet expectations.

#### 6.1 View Debug Logs

In the debug preview interface, after a conversation between the user and the AI, you can see the "Log" icon button and view the prompt log.

![Debug Log Entry](../../../../.gitbook/assets/日志.png)

#### 6.2 Trace Debug History

On the main interface for app construction, you can see "Logs and Annotations" in the left navigation bar to view the complete logs.

![View Prompt Log in Logs and Annotations Interface](../../../../.gitbook/assets/12.png)

{/*
Contributing Section
DO NOT edit this section!
It will be automatically generated by the script.
*/}

<CardGroup cols="2">
    <Card
        title="Edit this page"
        icon="pen-to-square"
        href="https://github.com/langgenius/dify-docs/edit/main/en/learn-more/prompt-engineering/prompt-engineering-1/README.mdx"
    >
        Help improve our documentation by contributing directly
    </Card>
    <Card
        title="Report an issue"
        icon="github"
        href="https://github.com/langgenius/dify-docs/issues/new?title=Documentation%20Issue%3A%20&body=%23%23%20Issue%20Description%0A%3C%21--%20Please%20briefly%20describe%20the%20issue%20you%20found%20--%3E%0A%0A%23%23%20Page%20Link%0Ahttps%3A%2F%2Fgithub.com%2Flanggenius%2Fdify-docs%2Fblob%2Fmain%2Fen/learn-more/prompt-engineering/prompt-engineering-1%2FREADME.mdx%0A%0A%23%23%20Suggested%20Changes%0A%3C%21--%20If%20you%20have%20specific%20suggestions%20for%20changes%2C%20please%20describe%20them%20here%20--%3E%0A%0A%3C%21--%20Thank%20you%20for%20helping%20improve%20our%20documentation%21%20--%3E"
    >
        Found an error or have suggestions? Let us know
    </Card>
</CardGroup>
