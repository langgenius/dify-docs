---
title: "Agent"
description: ""
icon: "robot"
---

Agents are chat-style apps where the model can **reason through a task**, **decide what to do next**, and **call tools when needed** to complete the user's request.

Unlike chatflows that follow predefined conversation paths, agents adapt their approach to the situation at hand. Compared to chatbots, agents can autonomously use available tools to complete tasks.

<Info>
    Agents maintain conversation memory (up to 2,000 tokens or 500 messages) to provide context-aware responses across multiple chat turns.
</Info>

## Choose a Model

We recommend selecting reasoning-strong models that natively support tool calling. To test and compare different models (up to 4) simultaneously, click **Debug as Multiple Models**.

<Info>
    Dify automatically applies the appropriate agent mode:

        - **Function Calling** for models with native tool support

        - **ReAct** (prompt-based) for others
</Info>

If you expect images, documents, or audio as inputs, choose a model that supports the corresponding modalities. Currently, video inputs are not supported.

After selection, you can adjust model parameters to control how it generates responses. Available parameters and presets vary by model. 

To balance answer quality with response time and token cost, adjust **Maximum Iterations** in **Agent Settings** to limit the number of reasoning loops (thought processes and tool actions) the model can perform for a single request.

## Write the Instruction

Write clear, specific prompts to improve the model's reasoning, decision-making, and response quality. Here are some best practices:

- **Define the persona**: Clearly state the agent's role and expertise.

- **Specify tool usage**: Guide the agent on when and how to use available tools.

- **Outline the workflow**: Break down complex tasks into logical, step-by-step instructions.

- **Specify output format**: Define the desired structure, length, or format of the response.

- **Set constraints**: Specify what the agent should avoid and when it needs to ask clarifying questions.

### Inject Dynamic User Inputs into Instructions

If you need your agent to adapt to different users or contexts without rewriting the prompt each time, use variables to collect necessary inputs upfront and inject them directly into the instructions at runtime.

This customizes the agent's behavior for each conversation while gathering required details in a structured way to reduce back-and-forth clarifications.

<Tip>
    While writing instructions, type `/` > **New Variable** to quickly insert a named placeholder. You can configure the details in the **Variables** section after you finish drafting.
</Tip>

<Tabs>
    <Tab title="Short Text">
    Accepts up to 256 characters. Use it for names, email addresses, titles, or any brief text input that fits on a single line.
    </Tab>

    <Tab title="Paragraph">
    Allows long-form text without length restrictions. It gives users a multi-line text area for detailed responses or descriptions.
    </Tab>
    <Tab title="Select">
    Displays a dropdown menu with predefined options. Users can choose only from the listed options, ensuring data consistency and preventing invalid inputs.
    </Tab>

    <Tab title="Number">
    Restricts input to numerical values only—ideal for quantities, ratings, IDs, or any data requiring mathematical processing.
    </Tab>

    <Tab title="Checkbox">
    Provides a simple yes/no option. When a user checks the box, the output is `true`; otherwise, it's `false`. Use it for confirmations or any case that requires a binary choice. 
    </Tab>

    <Tab title="API-based Variable">

    See [API Extension]() for details.

    </Tab>

</Tabs>

<Info>
    The **Label Name** is displayed to your end users.
</Info>

### Generate or Improve Instructions with AI

If you're unsure where to start or want to refine existing instructions, click **Generate** to let an LLM help you craft the prompt.

Describe what you want from scratch, or reference the `current_prompt` variable and specify what to improve.

For more targeted results, provide examples of your desired output format in **Ideal Output**.

Each generation is saved as a new version, so you can experiment freely and switch between versions to compare results.

## Add Tools

Add [tools](/en/use-dify/build/tool) to enable the agent to interact with external services and APIs for tasks that require real-time data or actions beyond text generation, such as searching the web and querying databases.

The agent decides when and which tools to use based on each query. To guide this more precisely, mention specific tool names in your instructions and describe when they should be used.

<img src="/images/tool_name.png" alt="Tool Name" width="400" />

**Configure Authentication**

If a tool requires authentication, click **Set up Credentials** and choose an authorization method:

    - **Workspace Credential**: Select an existing credential or create a new one. Your end users won't need to authorize manually.

        <Info>
          To change the default, go to the **Tools** or **Plugins** page.
        </Info>

    - **End-user Credential**: Require end users to provide their own credentials when using the app.

## Add Knowledge as Context

If your agent needs domain-specific information to generate more accurate and informed responses, add relevant knowledge bases so it can retrieve content related to user queries.

Once added, knowledge bases work automatically—you don't need to mention them in your instructions. The agent evaluates each user query against your knowledge base descriptions and decides whether retrieval is needed. 

**The more detailed your knowledge base description, the better the agent can determine relevance**, leading to more accurate and targeted retrieval.

To narrow retrieval to specific documents, enable automatic or manual metadata filtering. See [Manage Metadata](/en/use-dify/knowledge/metadata) for details.

By default, citations are shown alongside the response when knowledge is referenced. You can turn this off by disabling **Citation and Attributions** in **Features**.