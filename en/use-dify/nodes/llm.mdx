---
title: "LLM"
description: "Invoke large language models for text generation and analysis"
icon: "brain"
---

The LLM node calls a large language model (LLM) to generate responses based on your instructions and inputs from upstream nodes.

You can **inject context** to build RAG applications, enable **conversation memory** (in Chatflows) for more coherent responses to follow-up questions, force **structured JSON outputs** for reliable data extraction, let the LLM **call external tools** to complete complex tasks, etc.

## Choose a Model

Choose a model that best fits your task from configured providers. [Model tags](/en/use-dify/workspace/model-providers#model-tag-reference) highlight key capabilities and can help you decide.

After selection, you can adjust model parameters to control how it generates responses. Available parameters and presets vary by model.

## Write Instructions

Instruct the model on how to process inputs and generate responses. Reference variables by typing `/` or `{`.

The prompt interface adapts based on model type. Chat models support role-based prompts (System/User/Assistant), while completion models use simple text continuation.

<Info>
    Different models may have varying requirements for prompt structure and role usage. For best practices, refer to your model provider's documentation.
</Info>

If you're unsure where to start or want to refine existing instructions, try our AI-assisted prompt generator.

**Jinja2 Templating**

You can create dynamic prompts using [Jinja2](https://jinja.palletsprojects.com/en/stable/) syntax. For example, use conditionals to customize instructions based on variable values.

<Accordion title="Jinja2 Example: Conditional Prompt by User Level">
```jinja2 wrap
You are a 
{% if user_level == "beginner" %}patient and friendly 
{% elif user_level == "intermediate" %}professional and efficient 
{% else %}senior expert-level 
{% endif %} assistant.

{% if user_level == "beginner" %} 
Please explain in simple and easy-to-understand language. Provide examples when necessary. Avoid using technical jargon. 
{% elif user_level == "intermediate" %} You may use some technical terms, but provide appropriate explanations. Offer practical advice and best practices. 
{% else %} You may delve into technical details and use professional terminology. Focus on advanced use cases and optimization solutions. 
{% endif %}

User question: {{ query }}
```
</Accordion>

In default mode, you'd need to send all possible instructions to the model, describe the conditions, and let it decide which to follow—an approach that's often unreliable. 

With Jinja2, only the instructions matching the defined conditions are sent, ensuring predictable behavior and reducing token usage.

## Add Context

Inject additional information through `Context` to reduce hallucination and improve response accuracy. After specifying a context variable, reference it in your instruction using `{{Context}}`.

A typical pattern: [pass retrieval results from a knowledge retrieval node](/en/use-dify/nodes/knowledge-retrieval#use-with-llm-nodes) for Retrieval-Augmented Generation (RAG).

## Enable Conversation Memory (Chatflows Only)

<Note>
    Memory is node-specific and doesn't persist between different conversations.
</Note>

Enable `Memory` to pass recent chat history to the LLM, so it can answer follow-up questions with better conversational context. A **User** message will be automatically added to pass the current user query and any uploaded files.

**Window Size** controls how many recent exchanges to retain. For example, `5` keeps the last 5 user-message and LLM-response pairs.

For reasoning models, you can toggle on **Enable Reasoning Tag Separation** to exclude the model's thinking process from conversation memory. Only the final response will be stored.

## Add Tools

Add [tools](/en/use-dify/build/tool) to enable the model to interact with external services and APIs. This is useful when tasks require real-time data or actions beyond text generation, like web searches or database queries.

<Info>
    **Native vs. Prompt-based Tool Calling**

        - Models **with native support** (marked with a `Tool Call` tag) automatically decide when to use a tool, format requests correctly, and interpret results.

        - Models **without native support** can still call tools via prompt engineering. When adding a tool to these models, apply the suggested ReAct prompt to guide tool usage.
</Info>

**Configure Authentication**

If a tool requires authentication, click **Set up Credentials** and choose an authorization method:

    - **Workspace Credential**: Select an existing credential or create a new one. Your end users won't need to authorize manually.

        <Info>
          To change the default, go to the **Tools** or **Plugins** page.
        </Info>

    - **End-user Credential**: Require end users to provide their own credentials when using the app.

**Adjust Max Iterations**

**Max Iterations** limits the maximum number of reasoning loops (thought processes and tool actions) the model can perform for a single request. 

Increase this value for complex, multi-step tasks that require extensive reasoning or multiple tool calls. Higher values increase latency and token costs.

## Analyze Image Inputs

If the selected model is vision-capable, enable **Vision** and select a variable as the vision input to let the model interpret and analyze images.

<Note>
    Do not reference image variables in the instruction, which may cause errors.
</Note>

**Resolution** controls the detail level at which the model processes the image:

- **High**: Better accuracy for complex images but uses more tokens

- **Low**: Faster processing with fewer tokens for simple images

## Display Thinking and Tool Calling Processes

Display the model's thinking process or tool calls—if supported—alongside the final response to end users by referencing the `generations` variable in the answer node (Chatflows) or output node (Workflows).

The `text` variable contains only the final answer.

## Force Structured Output

While you can ask the model to generate responses in a specific format through instructions, responses may be inconsistent. 

For more reliable formatting, enable structured output to enforce a defined JSON schema. This also lets you directly reference specific fields in downstream nodes.

<Note>
    For models without native JSON support, Dify includes the schema in the prompt, but strict adherence is not guaranteed.
</Note>

1. Toggle on **Structured** next to **Output Variables**. The `structured_output` variable will appear at the end of the output variable list.

2. Click **Configure** to define the output schema using one of the following methods.

    - **Visual Editor**: Define simple structures with a no-code interface. The corresponding JSON schema is generated automatically.

    - **JSON Schema**: Directly write schemas for complex structures with nested objects, arrays, or validation rules.

    - **AI Generation**: Describe needs in natural language and let AI generate the schema.

    - **JSON Import**: Paste an existing JSON object to automatically generate the corresponding schema.

## Handle Errors

Configure **automatic retries** for temporary issues (like network glitches) and a fallback **error handling strategy** to keep the workflow running if errors persist.