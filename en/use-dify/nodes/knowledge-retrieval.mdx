---
title: Knowledge Retrieval
icon: "database"
---

## Introduction

You can use the Knowledge Retrieval node to integrate existing knowledge bases into your Chatflows or Workflows. The node searches specific knowledge for information relevant to queries and outputs results as contextual content for use in downstream nodes (e.g., LLM).

Below is an example of using the Knowledge Retrieval node in a Chatflow:

1. The **User Input** node collects the user query.

2. The **Knowledge Retrieval** node searches the selected knowledge base(s) for content related to the user query and outputs the retrieval results.

3. The **LLM** node generates a response based on both the user query and retrieved knowledge.

4. The **Answer** node returns the LLM's response to the user.

![Knowledge Retrieval Node Use Case](/images/knowledge_retrieval_node_example.png)

<Info>
    Before using the Knowledge Retrieval node, make sure at least one knowledge base is available. To learn about creating knowledge bases, see [Knowledge](/en/use-dify/knowledge/readme#create-knowledge).
</Info>

<Note>
    On Dify Cloud, knowledge retrieval operations are subject to rate limits based on the subscription plan. For more information, see [Knowledge Request Rate Limit](/en/use-dify/knowledge/knowledge-request-rate-limit).
</Note>

## Configuration

To make the Knowledge Retrieval node work properly, you need to specify:

- *What* it should search for (the query)

- *Where* it should search (the knowledge base)

- *How* to process the retrieval results (the node-level retrieval settings)

You can also use document metadata to enable filter-based searches and further improve retrieval precision.

### Specify the Query

Provide the query content that the node should search for in the selected knowledge base(s).

- **Query Text**: Select a text variable. For example, use `userinput.query` to reference user input in Chatflows, or a custom text-type user input variable in Workflows.

- **Query Images**: Select an image variable, e.g., the image(s) uploaded by the user through a User Input node, to search by image. The image size limit is 2 MB.

    <Tip>
        For self-hosted deployments, you can adjust the image size limit via the environment variable `ATTACHMENT_IMAGE_FILE_SIZE_LIMIT`.
    </Tip>

    <Info>
        The **Query Images** option is available only when at least one multimodal knowledge base are added. 

        Such knowledge bases are marked with a **Vision** icon, indicating that they are using a multimodal embedding model.
    </Info>

### Select Knowledge to Search

Add one or more existing knowledge bases for the node to search for content relevant to the query content.

When multiple knowledge bases are added, knowledge is first retrieved from all of them simultaneously, then combined and processed according to the [node-level retrieval settings](#configure-node-level-retrieval-settings).

<Info>
   Knowledge bases marked with a **Vision** icon support cross-modal retrieval—retrieving both text and images based on semantic relevance.
</Info>

<Tip>
    Click the **Edit** icon next to any added knowledge base to modify its settings directly within the Knowledge Retrieval node. 
    
    To learn more about these settings, see [Manage Knowledge Settings](/en/use-dify/knowledge/manage-knowledge/introduction).
</Tip>

### Configure Node-Level Retrieval Settings

Further fine-tune how the node processes retrieval results after they are fetched from the knowledge base(s).

<Info>
    There are two layers of retrieval settings—the knowledge base level and the knowledge retrieval node level. 
    
    Think of them as two consecutive filters: the knowledge base settings determine the initial pool of results, and the node settings further rerank the results or narrow down the pool.
</Info>

- **Rerank Settings**

    - **Weighted Score**: The relative weight between semantic similarity and keyword matching during reranking. Higher semantic weight favors meaning relevance, while higher keyword weight favors exact matches.

        <Info>
            **Weighted Score** is available only when all added knowledge bases are high-quality ones.
        </Info>

    - **Rerank Model**: The rerank model to re-score and reorder all the results based on their relevance to the query.

    <Note>
        If any multimodal knowledge bases are added, select a multimodal rerank model (marked with a **Vision** icon) as well. Otherwise, retrieved images will be excluded from reranking and the final output.
    </Note>

- **Top K**: The maximum number of top results to return after reranking. When a rerank model is selected, this value will be automatically adjusted based on the model's maximum input capacity (how much text the model can process at once).

- **Score Threshold**: The minimum similarity score for returned results. Results scoring below this threshold are excluded. Use higher thresholds for stricter relevance or lower thresholds to include broader matches.

### Enable Metadata Filtering

Use existing document metadata to restrict retrieval to specific documents within your knowledge base, improving retrieval precision.

With metadata filtering enabled, the Knowledge Retrieval node only searches documents that match the specified metadata conditions, rather than searching across the entire knowledge base. This is especially useful for targeted searching in large and diverse knowledge bases.

<Info>
    To learn more about creating and managing document metadata, see [Metadata](/en/use-dify/knowledge/metadata).
</Info>

## Output

The Knowledge Retrieval node outputs the retrieval results as a variable named `result`, which is an array of retrieved document chunks containing their content, metadata, title, and other attributes.

When the retrieval results contain image attachments, the `result` variable also includes a field named `files` containing image metadata.

## Use with LLM Nodes

To use the retrieval results as context to answer user questions in an LLM node:

1. In the **Context** field, select the `result` variable of the Knowledge Retrieval node.

2. In the prompt field, reference both the `Context` variable and the user input variable (e.g., `userinput.query` in Chatflows).

3. (Optional) If the LLM supports vision capabilities (marked with a **Vision** icon), enable **Vision** to let it interpret the retrieved images.

    <Info>
        Once **Vision** is enabled, the LLM automatically processes the retrieved images. You don't need to manually reference the `Context` variable again in the **Vision** input field.
    </Info>

<img src="/images/llm_node_configuration_example.png" alt="LLM Node Configuration Example" width="400"/>
