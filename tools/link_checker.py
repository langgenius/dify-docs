#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
é“¾æ¥æ£€æŸ¥å·¥å…·ï¼šæ£€æŸ¥æ–‡æ¡£ä¸­çš„å†…éƒ¨é“¾æ¥æ˜¯å¦æœ‰æ•ˆ

è¿™ä¸ªè„šæœ¬ç”¨äºæ‰«æMintlifyæ–‡æ¡£ä¸­çš„å†…éƒ¨é“¾æ¥ï¼Œå¹¶éªŒè¯å®ƒä»¬æ˜¯å¦æ­£ç¡®æŒ‡å‘å­˜åœ¨çš„æ–‡æ¡£ã€‚
æ”¯æŒç›¸å¯¹è·¯å¾„é“¾æ¥å’Œç»å¯¹è·¯å¾„é“¾æ¥ã€‚
"""

import os
import re
import json
import argparse
from pathlib import Path
from collections import defaultdict
from urllib.parse import urlparse

class LinkChecker:
    def __init__(self, docs_dir, docs_json_path, base_lang="zh-hans"):
        self.docs_dir = Path(docs_dir)
        self.docs_json_path = Path(docs_json_path)
        self.base_lang = base_lang
        self.valid_paths = set()
        self.external_links = set()
        self.documents = {}
        self.broken_links = defaultdict(list)
        
        # åŠ è½½docs.jsonæ–‡ä»¶ä»¥è·å–æœ‰æ•ˆè·¯å¾„
        self._load_docs_json()
        # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        self._load_documents()
    
    def _load_docs_json(self):
        """ä»docs.jsonåŠ è½½æœ‰æ•ˆè·¯å¾„ä¿¡æ¯"""
        with open(self.docs_json_path, 'r', encoding='utf-8') as f:
            docs_data = json.load(f)
        
        def process_pages(items, prefix=""):
            """é€’å½’å¤„ç†docs.jsonä¸­çš„é¡µé¢å±‚çº§ç»“æ„"""
            for item in items:
                if isinstance(item, dict):
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²URL (ç”¨äºå¤–éƒ¨é“¾æ¥)
                    if isinstance(item.get('pages'), str):
                        if item['pages'].startswith('http'):
                            self.external_links.add(item['pages'])
                        else:
                            self.valid_paths.add(item['pages'])
                    
                    # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ç±»å‹çš„pages
                    if 'pages' in item and isinstance(item['pages'], (list, dict)):
                        process_pages(item['pages'], prefix)
                    
                    # å¤„ç†groupæƒ…å†µä¸‹çš„pages
                    if 'group' in item and 'pages' in item:
                        process_pages(item['pages'], prefix)
                elif isinstance(item, str):
                    # ç›´æ¥æ˜¯æ–‡æ¡£è·¯å¾„
                    if item.startswith('http'):
                        self.external_links.add(item)
                    else:
                        self.valid_paths.add(item)
        
        # å¤„ç†navigationéƒ¨åˆ†
        if 'navigation' in docs_data and 'languages' in docs_data['navigation']:
            for lang in docs_data['navigation']['languages']:
                if 'tabs' in lang:
                    for tab in lang['tabs']:
                        if 'groups' in tab:
                            for group in tab['groups']:
                                if 'pages' in group:
                                    process_pages(group['pages'])
        
        print(f"ä»docs.jsonä¸­åŠ è½½äº† {len(self.valid_paths)} ä¸ªæœ‰æ•ˆæ–‡æ¡£è·¯å¾„")
    
    def _load_documents(self):
        """åŠ è½½æ–‡æ¡£ç›®å½•ä¸‹çš„æ‰€æœ‰.mdxå’Œ.mdæ–‡ä»¶"""
        for ext in ['.mdx', '.md']:
            for file_path in self.docs_dir.glob(f'**/*{ext}'):
                rel_path = file_path.relative_to(self.docs_dir)
                path_str = str(rel_path).replace('\\', '/')
                # ç§»é™¤æ‰©å±•åä»¥åŒ¹é…docs.jsonä¸­çš„è·¯å¾„
                path_without_ext = os.path.splitext(path_str)[0]
                self.documents[path_without_ext] = file_path
                
                # ä¹Ÿæ·»åŠ åˆ°æœ‰æ•ˆè·¯å¾„é›†åˆä¸­
                self.valid_paths.add(path_without_ext)
        
        print(f"æ‰«æåˆ° {len(self.documents)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    
    def _is_external_link(self, link):
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¤–éƒ¨é“¾æ¥"""
        parsed = urlparse(link)
        return bool(parsed.netloc) or link.startswith('http')
    
    def _is_valid_internal_link(self, link, base_path=None):
        """æ£€æŸ¥å†…éƒ¨é“¾æ¥æ˜¯å¦æœ‰æ•ˆ"""
        # å»é™¤é”šç‚¹å’ŒæŸ¥è¯¢å‚æ•°
        link = link.split('#')[0].split('?')[0]
        
        # è·³è¿‡ç©ºé“¾æ¥
        if not link:
            return True
        
        # å¦‚æœæ˜¯å®Œæ•´çš„è·¯å¾„
        if link in self.valid_paths:
            return True
        
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ ¹æ®åŸºç¡€è·¯å¾„è®¡ç®—
        if base_path and not link.startswith('/'):
            # è·å–åŸºç¡€ç›®å½•
            base_dir = os.path.dirname(base_path)
            # è®¡ç®—ç»å¯¹è·¯å¾„
            if base_dir:
                abs_path = os.path.normpath(os.path.join(base_dir, link))
            else:
                abs_path = link
            
            # æ›¿æ¢Windowsè·¯å¾„åˆ†éš”ç¬¦
            abs_path = abs_path.replace('\\', '/')
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æœ‰æ•ˆè·¯å¾„ä¸­
            return abs_path in self.valid_paths
        
        # å¦‚æœæ˜¯ä»¥/å¼€å¤´çš„ç»å¯¹è·¯å¾„
        if link.startswith('/'):
            # å»é™¤å¼€å¤´çš„æ–œæ 
            clean_link = link[1:]
            return clean_link in self.valid_paths
        
        return False
    
    def extract_links_from_file(self, file_path):
        """ä»æ–‡ä»¶ä¸­æå–é“¾æ¥"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        links = []
        
        # æå–Markdowné“¾æ¥ [text](url)
        md_links = re.findall(r'\[.+?\]\((.+?)\)', content)
        links.extend(md_links)
        
        # æå–HTMLé“¾æ¥ href="url"
        html_links = re.findall(r'href=[\'"](.+?)[\'"]', content)
        links.extend(html_links)
        
        # æå–Cardç»„ä»¶é“¾æ¥ href="url"
        card_links = re.findall(r'<Card.+?href=[\'"](.+?)[\'"]', content, re.DOTALL)
        links.extend(card_links)
        
        return links
    
    def check_links(self, specific_file=None):
        """æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£ä¸­çš„é“¾æ¥"""
        if specific_file:
            # åªæ£€æŸ¥ç‰¹å®šæ–‡ä»¶
            if os.path.exists(specific_file):
                rel_path = os.path.relpath(specific_file, self.docs_dir)
                path_without_ext = os.path.splitext(rel_path)[0].replace('\\', '/')
                self._check_file_links(specific_file, path_without_ext)
            else:
                print(f"é”™è¯¯: æ–‡ä»¶ {specific_file} ä¸å­˜åœ¨")
        else:
            # æ£€æŸ¥æ‰€æœ‰æ–‡æ¡£
            for path_without_ext, file_path in self.documents.items():
                self._check_file_links(file_path, path_without_ext)
        
        # è¾“å‡ºç»“æœ
        self._print_results()
    
    def _check_file_links(self, file_path, doc_path):
        """æ£€æŸ¥å•ä¸ªæ–‡ä»¶ä¸­çš„é“¾æ¥"""
        links = self.extract_links_from_file(file_path)
        
        for link in links:
            # è·³è¿‡å¤–éƒ¨é“¾æ¥å’Œé”šç‚¹é“¾æ¥
            if self._is_external_link(link) or link.startswith('#'):
                continue
            
            # æ£€æŸ¥å†…éƒ¨é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
            if not self._is_valid_internal_link(link, doc_path):
                self.broken_links[doc_path].append(link)
    
    def _print_results(self):
        """æ‰“å°æ£€æŸ¥ç»“æœ"""
        if not self.broken_links:
            print("âœ… æ‰€æœ‰é“¾æ¥æ£€æŸ¥å®Œæ¯•ï¼Œæœªå‘ç°æ— æ•ˆé“¾æ¥ï¼")
            return
        
        print("\nğŸ” é“¾æ¥æ£€æŸ¥ç»“æœ:")
        print("=" * 80)
        
        total_broken = sum(len(links) for links in self.broken_links.values())
        print(f"å…±å‘ç° {total_broken} ä¸ªæ— æ•ˆé“¾æ¥ï¼Œåˆ†å¸ƒåœ¨ {len(self.broken_links)} ä¸ªæ–‡æ¡£ä¸­")
        print("=" * 80)
        
        for doc, links in self.broken_links.items():
            print(f"\nğŸ“„ æ–‡æ¡£: {doc}")
            for link in links:
                print(f"   âŒ æ— æ•ˆé“¾æ¥: {link}")
        
        print("\n" + "=" * 80)

def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥Mintlifyæ–‡æ¡£ä¸­çš„å†…éƒ¨é“¾æ¥')
    parser.add_argument('--docs-dir', type=str, required=True, help='æ–‡æ¡£æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--docs-json', type=str, required=True, help='docs.jsonæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--file', type=str, help='è¦æ£€æŸ¥çš„ç‰¹å®šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--lang', type=str, default='zh-hans', help='åŸºç¡€è¯­è¨€ï¼ˆé»˜è®¤: zh-hansï¼‰')
    
    args = parser.parse_args()
    
    checker = LinkChecker(args.docs_dir, args.docs_json, args.lang)
    checker.check_links(args.file)

if __name__ == '__main__':
    main()
