#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
é’ˆå¯¹ç‰¹å®šæ–‡æ¡£çš„é“¾æ¥æ£€æŸ¥å·¥å…·
"""

import os
import re
import json
from pathlib import Path
from urllib.parse import urlparse

def is_external_link(link):
    """æ£€æŸ¥æ˜¯å¦ä¸ºå¤–éƒ¨é“¾æ¥"""
    parsed = urlparse(link)
    return bool(parsed.netloc) or link.startswith('http')

def load_valid_paths(docs_json_path):
    """ä»docs.jsonåŠ è½½æœ‰æ•ˆè·¯å¾„ä¿¡æ¯"""
    valid_paths = set()
    external_links = set()
    
    with open(docs_json_path, 'r', encoding='utf-8') as f:
        docs_data = json.load(f)
    
    def process_pages(items, prefix=""):
        """é€’å½’å¤„ç†docs.jsonä¸­çš„é¡µé¢å±‚çº§ç»“æ„"""
        for item in items:
            if isinstance(item, dict):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²URL (ç”¨äºå¤–éƒ¨é“¾æ¥)
                if isinstance(item.get('pages'), str):
                    if item['pages'].startswith('http'):
                        external_links.add(item['pages'])
                    else:
                        valid_paths.add(item['pages'])
                
                # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ç±»å‹çš„pages
                if 'pages' in item and isinstance(item['pages'], (list, dict)):
                    process_pages(item['pages'], prefix)
                
                # å¤„ç†groupæƒ…å†µä¸‹çš„pages
                if 'group' in item and 'pages' in item:
                    process_pages(item['pages'], prefix)
            elif isinstance(item, str):
                # ç›´æ¥æ˜¯æ–‡æ¡£è·¯å¾„
                if item.startswith('http'):
                    external_links.add(item)
                else:
                    valid_paths.add(item)
    
    # å¤„ç†navigationéƒ¨åˆ†
    if 'navigation' in docs_data and 'languages' in docs_data['navigation']:
        for lang in docs_data['navigation']['languages']:
            if 'tabs' in lang:
                for tab in lang['tabs']:
                    if 'groups' in tab:
                        for group in tab['groups']:
                            if 'pages' in group:
                                process_pages(group['pages'])
    
    print(f"ä»docs.jsonä¸­åŠ è½½äº† {len(valid_paths)} ä¸ªæœ‰æ•ˆæ–‡æ¡£è·¯å¾„")
    return valid_paths, external_links

def extract_links_from_file(file_path):
    """ä»æ–‡ä»¶ä¸­æå–é“¾æ¥"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    links = []
    
    # æå–Markdowné“¾æ¥ [text](url)
    md_links = re.findall(r'\[.+?\]\((.+?)\)', content)
    links.extend(md_links)
    
    # æå–HTMLé“¾æ¥ href="url"
    html_links = re.findall(r'href=[\'"](.+?)[\'"]', content)
    links.extend(html_links)
    
    # æå–Cardç»„ä»¶é“¾æ¥ href="url"
    card_links = re.findall(r'<Card.+?href=[\'"](.+?)[\'"]', content, re.DOTALL)
    links.extend(card_links)
    
    return links

def is_valid_internal_link(link, base_path, valid_paths):
    """æ£€æŸ¥å†…éƒ¨é“¾æ¥æ˜¯å¦æœ‰æ•ˆ"""
    # å»é™¤é”šç‚¹å’ŒæŸ¥è¯¢å‚æ•°
    link = link.split('#')[0].split('?')[0]
    
    # è·³è¿‡ç©ºé“¾æ¥
    if not link:
        return True
    
    # å¦‚æœæ˜¯å®Œæ•´çš„è·¯å¾„
    if link in valid_paths:
        return True
    
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ ¹æ®åŸºç¡€è·¯å¾„è®¡ç®—
    if base_path and not link.startswith('/'):
        # è·å–åŸºç¡€ç›®å½•
        base_dir = os.path.dirname(base_path)
        # è®¡ç®—ç»å¯¹è·¯å¾„
        if base_dir:
            abs_path = os.path.normpath(os.path.join(base_dir, link))
        else:
            abs_path = link
        
        # æ›¿æ¢Windowsè·¯å¾„åˆ†éš”ç¬¦
        abs_path = abs_path.replace('\\', '/')
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æœ‰æ•ˆè·¯å¾„ä¸­
        return abs_path in valid_paths
    
    # å¦‚æœæ˜¯ä»¥/å¼€å¤´çš„ç»å¯¹è·¯å¾„
    if link.startswith('/'):
        # å»é™¤å¼€å¤´çš„æ–œæ 
        clean_link = link[1:]
        return clean_link in valid_paths
    
    return False

def main():
    # é…ç½®
    docs_dir = '/Users/allen/Documents/dify-docs-mintlify'
    docs_json_path = '/Users/allen/Documents/dify-docs-mintlify/docs.json'
    target_file = '/Users/allen/Documents/dify-docs-mintlify/zh-hans/plugins/introduction.mdx'
    
    # åŠ è½½æœ‰æ•ˆè·¯å¾„
    valid_paths, external_links = load_valid_paths(docs_json_path)
    
    # è·å–ç›¸å¯¹è·¯å¾„
    rel_path = os.path.relpath(target_file, docs_dir)
    path_without_ext = os.path.splitext(rel_path)[0].replace('\\', '/')
    
    print(f"æ£€æŸ¥æ–‡ä»¶: {path_without_ext}")
    
    # æ£€æŸ¥é“¾æ¥
    links = extract_links_from_file(target_file)
    broken_links = []
    
    for link in links:
        # è·³è¿‡å¤–éƒ¨é“¾æ¥å’Œé”šç‚¹é“¾æ¥
        if is_external_link(link) or link.startswith('#'):
            continue
        
        # æ£€æŸ¥å†…éƒ¨é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
        if not is_valid_internal_link(link, path_without_ext, valid_paths):
            broken_links.append(link)
    
    # è¾“å‡ºç»“æœ
    if not broken_links:
        print("\nâœ… æ‰€æœ‰é“¾æ¥æ£€æŸ¥å®Œæ¯•ï¼Œæœªå‘ç°æ— æ•ˆé“¾æ¥ï¼")
    else:
        print("\nğŸ” é“¾æ¥æ£€æŸ¥ç»“æœ:")
        print("=" * 80)
        print(f"å…±å‘ç° {len(broken_links)} ä¸ªæ— æ•ˆé“¾æ¥")
        print("=" * 80)
        
        for link in broken_links:
            print(f"   âŒ æ— æ•ˆé“¾æ¥: {link}")

if __name__ == '__main__':
    main()
