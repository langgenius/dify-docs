---
title: LLM 配置と使用
---


### 1. 国内環境でOpenAIのプロキシサーバを利用するにはどうすればよいですか？

DifyはOpenAIのカスタムAPIドメイン機能をサポートしており、OpenAIと互換性のある大規模モデルAPIサーバを利用できます。コミュニティ版では、**設定 --> モデルプロバイダ --> OpenAI --> API編集**の入口に目標サーバのアドレスを入力するだけで使用できます。

### **2. 基本モデルをどのように選択しますか？**

* gpt-3.5-turbo  
  gpt-3.5-turboはgpt-3モデルシリーズのアップグレード版で、gpt-3よりも強力で、より複雑なタスクを処理できます。長文の理解やドキュメント間の推論において大幅な向上を遂げています。gpt-3.5-turboはより一貫性のある説得力のあるテキストを生成でき、要約、翻訳、創作においても大きな改善が見られます。  
  得意分野：**長文の理解、ドキュメント間の推論、要約、翻訳、創作。**

* gpt-4  
  gpt-4は最新かつ最強のトランスフォーマー言語モデルです。事前学習のパラメータ数が約200億に増加し、すべての言語タスクにおいて最高レベルの性能を発揮します。特に、長く複雑な応答を深く理解し生成するタスクにおいて優れています。gpt-4は抽象概念の理解やページを跨ぐ推論も可能で、人間の言語のすべての側面を扱えます。gpt-4は初の真の汎用言語理解システムで、人工知能分野のいかなる自然言語処理タスクもこなせます。  
  得意分野：**すべてのNLPタスク、言語理解、長文生成、ドキュメント間推論、抽象概念理解**  
  詳細は[ドキュメント](https://platform.openai.com/docs/models/overview)を参照してください。

### **3. なぜmax_tokensの設定を小さくすることを推奨するのですか？**

自然言語処理において、長いテキストの出力は通常、計算時間と計算リソースを多く必要とします。したがって、出力テキストの長さを制限することで、計算コストと計算時間をある程度削減できます。例えば、max_tokens=500と設定すると、テキストの最初の500トークンだけを考慮し、それを超える部分は破棄されます。これは、出力テキストの長さがLLMの許容範囲を超えないようにしつつ、計算リソースを効率的に利用し、モデルの実行効率を向上させるためです。さらに、max_tokensの制限により、プロンプトの長さが増加します。例えば、gpt-3.5-turboの制限は4097トークンで、max_tokens=4000と設定すると、プロンプトには97トークンしか残らず、超えるとエラーが発生します。

### **4. データセットの長文をどのように分割するのが合理的ですか？**

自然言語処理のいくつかのアプリケーションでは、通常、テキストを段落や文ごとに分割して、テキストの意味と構造情報をよりよく処理し理解します。最小分割単位は特定のタスクと技術実装によります。例えば：

* テキスト分類タスクでは、通常、テキストを文または段落ごとに分割します。
* 機械翻訳タスクでは、文または段落全体を分割単位とします。

最後に、実験と評価を行い、最適なembedding技術と分割単位を決定します。テストセットでのテストを通じて、異なる技術と分割単位の性能を比較し、最適なソリューションを選びます。

### 5. データセットの分割に使用する距離関数は何ですか？

[コサイン類似度](https://en.wikipedia.org/wiki/Cosine_similarity)を使用します。距離関数の選択は通常重要ではありません。OpenAIの埋め込みは長さ1に正規化されており、これは次のことを意味します：

点積のみを使用してコサイン類似度を少し速く計算できます。

コサイン類似度とユークリッド距離は同じランキングをもたらします。

* > 正規化された埋め込みベクトルを使用してコサイン類似度またはユークリッド距離を計算し、これらの類似度尺度に基づいてベクトルを並べ替えた場合、得られるランキング結果は同じになります。つまり、コサイン類似度またはユークリッド距離を使用してベクトル間の類似性を測定しても、ランキング結果は一貫しています。これは、正規化後のベクトルの長さはそれらの相対的関係に影響せず、方向情報のみが保持されるためです。したがって、正規化されたベクトルを使用した類似性測定では、異なる測定方法でも同じランキング結果が得られます。ベクトルが正規化された後、すべてのベクトルの長さは1にスケールされます。単位ベクトルは方向のみを記述し、長さは常に1だからです。_具体的な原理はChatGPTに質問できます。_

埋め込みベクトルが長さ1に正規化されると、2つのベクトル間のコサイン類似度の計算はそれらの点積に簡略化されます。正規化されたベクトルの長さがすべて1であるため、点積の結果はコサイン類似度の結果と同じです。点積計算は他の類似度測定（例えばユークリッド距離）よりも計算速度が速いため、正規化されたベクトルを使用した点積計算は計算効率を若干向上させます。

### 6. OpenAIキーを入力すると、「**校验失败： You exceeded your current quota, please check your plan and billing details。**」というエラーメッセージが表示されるのはなぜですか？

これは、OpenAIキーのアカウントが料金不足であることを示しています。OpenAIの公式サイトでチャージしてください。

### 7. OpenAIキーを使用してアプリ内で対話すると、次のようなエラーメッセージが表示されるのはなぜですか？

エラーメッセージ1：

```JSON
The server encountered an internal error and was unable to complete your request。Either the server is overloaded or there is an error in the application
```

エラーメッセージ2：

```JSON
Rate limit reached for default-gpt-3.5-turboin organization org-wDrZCxxxxxxxxxissoZb on requestsper min。 Limit: 3 / min. Please try again in 20s. Contact us through our help center   at help.openai.com   if you continue to haveissues. Please add a payment method toyour account to increase your rate limit.Visit https://platform.openai.com/account/billingto add a payment method.
```

公式インターフェースの呼び出し速度制限に達しているかどうかを確認してください。詳細は[OpenAI公式ドキュメント](https://platform.openai.com/docs/guides/rate-limits)を参照してください。

### 8. ユーザーが自分でデプロイした後、智聊が使用できず、次のエラーメッセージが表示される場合の対処法：「**Unrecognized request argument supplied:functions**」

まず、フロントエンドとバックエンドのバージョンが最新であり、フロントエンドとバックエンドのバージョンが一致しているかを確認してください。次に、このエラーはAzure OpenAIのキーを使用しているが、モデルのデプロイに失敗した場合にも発生する可能性があります。使用しているAzure OpenAIにモデルがデプロイされているか確認してください。gpt-3.5-turboモデルのバージョンは0613以上である必要があります。（0613以前のバージョンは智聊が使用するfunction call機能をサポートしていないため、使用できません）

### 9. OpenAIキーを設定するときに次のエラーが発生する場合の対処法：

```JSON
Error communicating with OpenAl: HTTPSConnectionPool(host='api.openai.com', port=443): Max retriesexceeded with url: /v1/chat/completions (Caused byNewConnectionError( <urllib3.connection.HTTPSConnection object at 0x7f0462ed7af0>; Failed toestablish a new connection: [Errno -3] Temporary failure in name resolution'))
```

通常、このエラーは環境にプロキシが設定されているために発生します。プロキシが設定されているかどうかを確認してください。

### 10. アプリ内でモデルを切り替える際に次のエラーメッセージが表示される場合の対処法：

```JSON
Anthropic: Error code: 400 - f'error': f'type': "invalid request error, 'message': 'temperature: range: -1 or 0..1)
```

各モデルのパラメータの範囲が異なるため、現在のモデルのパラメータ範囲に従って設定する必要があります。

### 11. 次のエラーメッセージが表示される場合の対処法：

```JSON
Query or prefix prompt is too long, you can reduce the preix prompt, or shrink the max token, or switch to a llm with a larger token limit size
```

編成ページのパラメータ設定で「最大トークン」の値を小さく設定してください。

### 12. Difyのデフォルトモデルは何ですか？オープンソースのモデルを使用できますか？

デフォルトモデルは**設定 - モデルプロバイダ**で設定できます。現在、OpenAI / Azure OpenAI / Anthropicなどのテキスト生成モデルをサポートしており、Hugging Face / Replicate / xinferenceでホストされているオープンソースモデルの接続もサポートしています。

### 13. コミュニティ版でデータセットの**Q&A分割モード**が常に待機中になるのはなぜですか？

使用している埋め込みモデルのAPIキーが速度制限に達しているかどうかを確認してください。

### 14. アプリ使用中に「Invalid token」エラーが発生する場合の対処法：

「Invalid token」エラーが発生した場合、以下の2つの解決方法を試してください：

* ブラウザのキャッシュ（クッキー、セッションストレージ、ローカルストレージ）をクリアします。スマートフォンを使用している場合は、対応するアプリのキャッシュをクリアして再度アクセスします。
* 新しいアプリURLを生成し、再度アクセスします。

### 15. データセットドキュメントのアップロードサイズ制限は何ですか？

現在、データセットドキュメントのアップロードは1つのドキュメントで最大15MB、ドキュメントの総数は100個に制限されています。ローカルデプロイ版でこの制限を調整する必要がある場合は、[ドキュメント](https://docs.dify.ai/v/ja-jp/learn-more/faq/install-faq#id-11-rkarubjonnodtasettodedokyumentowoappurdosurunosaizutowosuruha)を参照してください。

### 16. Claudeモデルを選択しているのに、なぜOpenAIの費用がかかるのですか？

Claudeは埋め込みモデルをサポートしていないため、埋め込みプロセスやその他の対話生成はデフォルトでOpenAIのキーを使用するため、OpenAIのクォータを消費します。また、**設定 - モデルプロバイダ**で他のデフォルト推論モデルと埋め込みモデルを設定することもできます。

### 17. データを使ってモデルの生成能力をより多く活用する方法はありますか？

データセットの使用はデータセットの説明に関係します。データセットの説明を明確に記述するようにします。具体的には[こちらのドキュメント](/ja-jp/guides/knowledge-base/readme)を参照してください。

### 18. データセットドキュメントがExcelの場合、どのように分割するのが良いですか？

最初の行にヘッダを設定し、後続の各行に内容を表示します。余分なヘッダ設定や複雑な形式の表を設定しないようにします。

以下の表の例のように、2行目のヘッダだけを保持し、1行目（表1）は余分なヘッダのため削除します。

<img
src="../../.gitbook/assets/table_delete_row1.png"
className="mx-auto"
alt=""
/>

### 19. ChatGPT Plusを購入したのに、なぜdifyでGPT4を使用できないのですか？

OpenAIのGPT4モデルAPIとChatGPT Plusは別々の製品で、料金も別々です。モデルAPIには独自の料金体系があります。詳細は[OpenAI料金ドキュメント](https://openai.com/pricing)を参照してください。支払いの申し込みにはカードの登録が必要で、カードを登録するとGPT3.5の権限が付与されますが、GPT4の権限はありません。GPT4の権限を得るには、一度の支払いの請求書が必要です。詳細は[OpenAI公式ドキュメント](https://platform.openai.com/account/billing/overview)を参照してください。

### 20. 他の埋め込みモデルを追加する方法はありますか？

Difyは以下の埋め込みモデル使用をサポートしており、設定ボックスで`Embeddings`タイプを選択するだけで使用できます。

* Azure
* LocalAI
* MiniMax
* OpenAI
* Replicate
* XInference

### 21. 自分が作成したアプリケーションをアプリケーションテンプレートとして設定する方法はありますか？

この機能はDify公式が提供するアプリケーションテンプレートで、クラウド版ユーザーの参考用です。自分が作成したアプリケーションをテンプレートとして設定することは現在サポートしていません。クラウド版を使用している場合、**ワークスペースに追加**または**カスタマイズ**して自分のアプリケーションとして使用できます。コミュニティ版を使用していて、チーム用にさらにアプリケーション テンプレートを作成する必要がある場合は、有料のテクニカル サポートについて、当社の商用化チーム (business@dify.ai) にお問い合わせください。

{/*
Contributing Section
DO NOT edit this section!
It will be automatically generated by the script.
*/}

---

[このページを編集する](https://github.com/langgenius/dify-docs/edit/main/ja-jp/learn-more/faq/llms-use-faq.mdx) | [問題を報告する](https://github.com/langgenius/dify-docs/issues/new?template=docs.yml)

